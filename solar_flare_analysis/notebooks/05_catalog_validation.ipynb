{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13cb0082",
   "metadata": {},
   "source": [
    "# Solar Flare Analysis: Validation Against Known Catalogs\n",
    "\n",
    "This notebook demonstrates how to validate our flare detection methods against known solar flare catalogs from official sources like NOAA SWPC.\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b997e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Add the project root to the path\n",
    "project_root = os.path.abspath('..')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import project modules\n",
    "from config import settings\n",
    "from src.data_processing.data_loader import load_goes_data, preprocess_xrs_data, remove_background\n",
    "from src.flare_detection.traditional_detection import (\n",
    "    detect_flare_peaks, define_flare_bounds, detect_overlapping_flares\n",
    ")\n",
    "from src.ml_models.flare_decomposition import FlareDecompositionModel, reconstruct_flares\n",
    "from src.validation.catalog_validation import (\n",
    "    download_noaa_flare_catalog, compare_detected_flares, \n",
    "    calculate_detection_quality, get_flare_class_distribution\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8d0e97",
   "metadata": {},
   "source": [
    "## Downloading Flare Catalogs\n",
    "\n",
    "First, let's download the NOAA SWPC flare catalog for a specific date range. This catalog contains officially reported solar flare events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea9661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define date range for validation\n",
    "start_date = '2022-06-01'\n",
    "end_date = '2022-06-30'\n",
    "\n",
    "# Create a directory for catalogs if it doesn't exist\n",
    "catalog_dir = os.path.join('..', 'data', 'catalogs')\n",
    "os.makedirs(catalog_dir, exist_ok=True)\n",
    "\n",
    "# Download NOAA SWPC flare catalog\n",
    "catalog_file = os.path.join(catalog_dir, f'noaa_flares_{start_date}_to_{end_date}.csv')\n",
    "\n",
    "try:\n",
    "    if os.path.exists(catalog_file):\n",
    "        print(f\"Loading existing catalog from {catalog_file}\")\n",
    "        catalog_flares = pd.read_csv(catalog_file, parse_dates=['start_time', 'end_time'])\n",
    "    else:\n",
    "        print(f\"Downloading NOAA SWPC flare catalog from {start_date} to {end_date}\")\n",
    "        catalog_flares = download_noaa_flare_catalog(start_date, end_date, output_file=catalog_file)\n",
    "    \n",
    "    # Display the catalog\n",
    "    print(f\"Downloaded {len(catalog_flares)} flare events\")\n",
    "    display(catalog_flares.head())\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading catalog: {e}\")\n",
    "    catalog_flares = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db0f26e",
   "metadata": {},
   "source": [
    "## Analyzing Catalog Flares\n",
    "\n",
    "Let's analyze the distribution of flare classes in the catalog:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca7ee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not catalog_flares.empty:\n",
    "    # Get flare class distribution\n",
    "    class_distribution = get_flare_class_distribution(catalog_flares)\n",
    "    \n",
    "    # Plot distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    class_distribution.plot(kind='bar', color='skyblue')\n",
    "    plt.xlabel('Flare Class')\n",
    "    plt.ylabel('Number of Flares')\n",
    "    plt.title('Distribution of Flare Classes in NOAA Catalog')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nSummary Statistics for Flares in Catalog:\")\n",
    "    print(f\"Total flares: {len(catalog_flares)}\")\n",
    "    print(f\"Date range: {catalog_flares['start_time'].min().date()} to {catalog_flares['start_time'].max().date()}\")\n",
    "    \n",
    "    # Duration statistics\n",
    "    catalog_flares['duration_minutes'] = (catalog_flares['end_time'] - catalog_flares['start_time']).dt.total_seconds() / 60\n",
    "    print(f\"Average flare duration: {catalog_flares['duration_minutes'].mean():.2f} minutes\")\n",
    "    print(f\"Median flare duration: {catalog_flares['duration_minutes'].median():.2f} minutes\")\n",
    "    print(f\"Maximum flare duration: {catalog_flares['duration_minutes'].max():.2f} minutes\")\n",
    "    \n",
    "    # Plot flare durations by class\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x='flare_class', y='duration_minutes', data=catalog_flares)\n",
    "    plt.xlabel('Flare Class')\n",
    "    plt.ylabel('Duration (minutes)')\n",
    "    plt.title('Flare Durations by Class')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5d0def",
   "metadata": {},
   "source": [
    "## Loading GOES XRS Data\n",
    "\n",
    "Now, let's load the GOES XRS data for the same time period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2139861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Locate data files for the validation period\n",
    "data_dir = settings.DATA_DIR\n",
    "\n",
    "# Look for files that match our date range\n",
    "data_files = []\n",
    "for file in os.listdir(data_dir):\n",
    "    if file.endswith('.nc'):\n",
    "        # Try to extract date from filename\n",
    "        try:\n",
    "            # GOES files often have dates in the format YYYYMMDD\n",
    "            date_match = re.search(r'\\d{8}', file)\n",
    "            if date_match:\n",
    "                file_date = datetime.strptime(date_match.group(), '%Y%m%d').date()\n",
    "                # Check if date is in our range\n",
    "                start_date_obj = datetime.strptime(start_date, '%Y-%m-%d').date()\n",
    "                end_date_obj = datetime.strptime(end_date, '%Y-%m-%d').date()\n",
    "                if start_date_obj <= file_date <= end_date_obj:\n",
    "                    data_files.append(file)\n",
    "        except Exception:\n",
    "            # If date extraction fails, include the file anyway\n",
    "            data_files.append(file)\n",
    "\n",
    "print(f\"Found {len(data_files)} data files matching date range\")\n",
    "\n",
    "# If no specific files found, use whatever is available\n",
    "if not data_files:\n",
    "    data_files = [f for f in os.listdir(data_dir) if f.endswith('.nc')]\n",
    "    print(f\"Using {len(data_files)} available data files\")\n",
    "\n",
    "if data_files:\n",
    "    # Process each file\n",
    "    all_flares = []\n",
    "    \n",
    "    for data_file in data_files[:3]:  # Limit to first 3 files for demo\n",
    "        file_path = os.path.join(data_dir, data_file)\n",
    "        print(f\"\\nProcessing {data_file}...\")\n",
    "        \n",
    "        # Load data\n",
    "        data = load_goes_data(file_path)\n",
    "        if data is None:\n",
    "            print(f\"Failed to load {data_file}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Preprocess B channel data\n",
    "        channel = 'B'\n",
    "        flux_col = f'xrs{channel.lower()}'\n",
    "        df = preprocess_xrs_data(data, channel=channel, remove_bad_data=True, interpolate_gaps=True)\n",
    "        \n",
    "        # Remove background\n",
    "        df_bg = remove_background(\n",
    "            df, \n",
    "            window_size=settings.BACKGROUND_PARAMS['window_size'],\n",
    "            quantile=settings.BACKGROUND_PARAMS['quantile']\n",
    "        )\n",
    "        \n",
    "        # Detect flares\n",
    "        peaks = detect_flare_peaks(\n",
    "            df, flux_col,\n",
    "            threshold_factor=settings.DETECTION_PARAMS['threshold_factor'],\n",
    "            window_size=settings.DETECTION_PARAMS['window_size']\n",
    "        )\n",
    "        \n",
    "        flares = define_flare_bounds(\n",
    "            df, flux_col, peaks['peak_index'].values,\n",
    "            start_threshold=settings.DETECTION_PARAMS['start_threshold'],\n",
    "            end_threshold=settings.DETECTION_PARAMS['end_threshold'],\n",
    "            min_duration=settings.DETECTION_PARAMS['min_duration'],\n",
    "            max_duration=settings.DETECTION_PARAMS['max_duration']\n",
    "        )\n",
    "        \n",
    "        # Add file information\n",
    "        flares['source_file'] = data_file\n",
    "        \n",
    "        # Add to list\n",
    "        all_flares.append(flares)\n",
    "        \n",
    "        print(f\"Detected {len(flares)} flares in {data_file}\")\n",
    "    \n",
    "    # Combine all detected flares\n",
    "    if all_flares:\n",
    "        detected_flares = pd.concat(all_flares, ignore_index=True)\n",
    "        print(f\"\\nTotal detected flares: {len(detected_flares)}\")\n",
    "        display(detected_flares.head())\n",
    "    else:\n",
    "        print(\"No flares detected\")\n",
    "        detected_flares = pd.DataFrame()\n",
    "else:\n",
    "    print(\"No data files found\")\n",
    "    detected_flares = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c319e274",
   "metadata": {},
   "source": [
    "## Comparing Detected Flares with Catalog\n",
    "\n",
    "Now let's compare our detected flares with the catalog flares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8daa17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not detected_flares.empty and not catalog_flares.empty:\n",
    "    # Make sure both dataframes have the required columns\n",
    "    required_columns = ['start_time', 'peak_time', 'end_time', 'peak_flux']\n",
    "    \n",
    "    if all(col in detected_flares.columns for col in required_columns) and \\\n",
    "       all(col in catalog_flares.columns for col in required_columns):\n",
    "        \n",
    "        # Filter catalog flares to match the time range of detected flares\n",
    "        min_time = detected_flares['start_time'].min()\n",
    "        max_time = detected_flares['end_time'].max()\n",
    "        \n",
    "        filtered_catalog = catalog_flares[\n",
    "            (catalog_flares['end_time'] >= min_time) & \n",
    "            (catalog_flares['start_time'] <= max_time)\n",
    "        ]\n",
    "        \n",
    "        print(f\"Comparing {len(detected_flares)} detected flares with {len(filtered_catalog)} catalog flares\")\n",
    "        \n",
    "        # Compare flares\n",
    "        comparison = compare_detected_flares(\n",
    "            detected_flares, \n",
    "            filtered_catalog, \n",
    "            time_tolerance='10min'  # Allow 10 minutes time difference\n",
    "        )\n",
    "        \n",
    "        # Display results\n",
    "        metrics = comparison['metrics']\n",
    "        print(\"\\nValidation Metrics:\")\n",
    "        print(f\"True Positives: {metrics['true_positives']}\")\n",
    "        print(f\"False Positives: {metrics['false_positives']}\")\n",
    "        print(f\"False Negatives: {metrics['false_negatives']}\")\n",
    "        print(f\"Precision: {metrics['precision']:.3f}\")\n",
    "        print(f\"Recall: {metrics['recall']:.3f}\")\n",
    "        print(f\"F1 Score: {metrics['f1_score']:.3f}\")\n",
    "        \n",
    "        # Additional quality metrics\n",
    "        quality_metrics = calculate_detection_quality(comparison)\n",
    "        if 'mean_time_diff' in quality_metrics:\n",
    "            print(f\"\\nAverage time difference: {quality_metrics['mean_time_diff']:.2f} minutes\")\n",
    "        if 'mean_flux_ratio' in quality_metrics:\n",
    "            print(f\"Average flux ratio (detected/catalog): {quality_metrics['mean_flux_ratio']:.2f}\")\n",
    "        \n",
    "        # Display matched flares\n",
    "        print(\"\\nSample of matched flares:\")\n",
    "        display(comparison['matched_flares'].head())\n",
    "        \n",
    "        # Plot precision-recall by flare class\n",
    "        if len(comparison['matched_flares']) > 0:\n",
    "            # Add flare class to matched flares\n",
    "            matched = comparison['matched_flares']\n",
    "            \n",
    "            # Create a list of all classes\n",
    "            flare_classes = ['A', 'B', 'C', 'M', 'X']\n",
    "            \n",
    "            # Calculate metrics by class\n",
    "            class_metrics = []\n",
    "            \n",
    "            for flare_class in flare_classes:\n",
    "                # Get catalog flares of this class\n",
    "                class_catalog = filtered_catalog[filtered_catalog['flare_class'] == flare_class]\n",
    "                \n",
    "                # Find corresponding detected flares\n",
    "                class_matched_idx = matched[matched['catalog_idx'].isin(class_catalog.index)]\n",
    "                \n",
    "                # Count TP, FP, FN\n",
    "                tp = len(class_matched_idx)\n",
    "                fn = len(class_catalog) - tp\n",
    "                \n",
    "                # Calculate metrics\n",
    "                precision = tp / (tp + 0) if tp > 0 else 0  # No false positives by class\n",
    "                recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "                \n",
    "                class_metrics.append({\n",
    "                    'class': flare_class,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'count': len(class_catalog)\n",
    "                })\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            class_metrics_df = pd.DataFrame(class_metrics)\n",
    "            \n",
    "            # Plot metrics by class\n",
    "            fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "            \n",
    "            # Bar plot for counts\n",
    "            ax2 = ax1.twinx()\n",
    "            ax2.bar(class_metrics_df['class'], class_metrics_df['count'], \n",
    "                   alpha=0.3, color='gray', label='Flare Count')\n",
    "            ax2.set_ylabel('Number of Flares')\n",
    "            \n",
    "            # Line plots for precision and recall\n",
    "            ax1.plot(class_metrics_df['class'], class_metrics_df['precision'], \n",
    "                    'o-', color='blue', label='Precision')\n",
    "            ax1.plot(class_metrics_df['class'], class_metrics_df['recall'], \n",
    "                    'o-', color='red', label='Recall')\n",
    "            \n",
    "            ax1.set_xlabel('Flare Class')\n",
    "            ax1.set_ylabel('Score')\n",
    "            ax1.set_ylim(0, 1.05)\n",
    "            ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "            \n",
    "            # Combine legends\n",
    "            lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "            ax1.legend(lines1 + lines2, labels1 + labels2, loc='lower right')\n",
    "            \n",
    "            plt.title('Detection Performance by Flare Class')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Display class metrics\n",
    "            display(class_metrics_df)\n",
    "    else:\n",
    "        missing_columns = [col for col in required_columns if col not in detected_flares.columns]\n",
    "        missing_columns += [col for col in required_columns if col not in catalog_flares.columns]\n",
    "        print(f\"Cannot compare flares. Missing columns: {missing_columns}\")\n",
    "else:\n",
    "    print(\"Cannot compare flares: either no detected flares or no catalog flares available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2f238a",
   "metadata": {},
   "source": [
    "## Analyzing Detection Performance\n",
    "\n",
    "Let's analyze the detection performance in more detail, especially for flares that were missed or falsely detected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13a2278",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'comparison' in locals() and all(key in comparison for key in ['unmatched_detected', 'unmatched_catalog']):\n",
    "    # Analyze unmatched detected flares (false positives)\n",
    "    false_positives = comparison['unmatched_detected']\n",
    "    if len(false_positives) > 0:\n",
    "        print(f\"\\nAnalysis of {len(false_positives)} False Positives (detected but not in catalog):\")\n",
    "        \n",
    "        # Calculate statistics\n",
    "        if 'peak_flux' in false_positives.columns:\n",
    "            print(f\"Mean peak flux: {false_positives['peak_flux'].mean():.2e} W/m²\")\n",
    "            print(f\"Median peak flux: {false_positives['peak_flux'].median():.2e} W/m²\")\n",
    "            print(f\"Max peak flux: {false_positives['peak_flux'].max():.2e} W/m²\")\n",
    "        \n",
    "        if 'start_time' in false_positives.columns and 'end_time' in false_positives.columns:\n",
    "            false_positives['duration'] = (false_positives['end_time'] - false_positives['start_time']).dt.total_seconds() / 60\n",
    "            print(f\"Mean duration: {false_positives['duration'].mean():.2f} minutes\")\n",
    "            print(f\"Median duration: {false_positives['duration'].median():.2f} minutes\")\n",
    "        \n",
    "        # Distribution of false positives by peak flux\n",
    "        if 'peak_flux' in false_positives.columns:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(false_positives['peak_flux'], bins=20, alpha=0.7, log=True)\n",
    "            plt.xlabel('Peak Flux (W/m²)')\n",
    "            plt.ylabel('Number of False Positives')\n",
    "            plt.title('Distribution of False Positive Flares by Peak Flux')\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            plt.xscale('log')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    # Analyze unmatched catalog flares (false negatives)\n",
    "    false_negatives = comparison['unmatched_catalog']\n",
    "    if len(false_negatives) > 0:\n",
    "        print(f\"\\nAnalysis of {len(false_negatives)} False Negatives (in catalog but not detected):\")\n",
    "        \n",
    "        # Distribution by flare class\n",
    "        if 'flare_class' in false_negatives.columns:\n",
    "            missed_class_dist = false_negatives['flare_class'].value_counts().sort_index()\n",
    "            print(\"\\nMissed flares by class:\")\n",
    "            for cls, count in missed_class_dist.items():\n",
    "                print(f\"  {cls}: {count}\")\n",
    "            \n",
    "            # Plot distribution\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            missed_class_dist.plot(kind='bar', color='tomato')\n",
    "            plt.xlabel('Flare Class')\n",
    "            plt.ylabel('Number of Missed Flares')\n",
    "            plt.title('Distribution of Missed Flares by Class')\n",
    "            plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # Duration of missed flares\n",
    "        if 'start_time' in false_negatives.columns and 'end_time' in false_negatives.columns:\n",
    "            false_negatives['duration'] = (false_negatives['end_time'] - false_negatives['start_time']).dt.total_seconds() / 60\n",
    "            print(f\"\\nMean duration of missed flares: {false_negatives['duration'].mean():.2f} minutes\")\n",
    "            print(f\"Median duration of missed flares: {false_negatives['duration'].median():.2f} minutes\")\n",
    "            \n",
    "            # Plot duration distribution\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.histplot(data=false_negatives, x='duration', hue='flare_class' if 'flare_class' in false_negatives.columns else None,\n",
    "                        kde=True, bins=20)\n",
    "            plt.xlabel('Duration (minutes)')\n",
    "            plt.ylabel('Number of Missed Flares')\n",
    "            plt.title('Duration Distribution of Missed Flares')\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cccdd6",
   "metadata": {},
   "source": [
    "## Validating ML-based Flare Separation\n",
    "\n",
    "Finally, let's validate our ML-based flare separation approach against overlapping flares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f0ea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'detected_flares' in locals() and not detected_flares.empty:\n",
    "    # Detect overlapping flares\n",
    "    overlapping = detect_overlapping_flares(detected_flares, min_overlap='2min')\n",
    "    print(f\"Detected {len(overlapping)} potentially overlapping flare pairs\")\n",
    "    \n",
    "    if overlapping:\n",
    "        # Load the ML model\n",
    "        print(\"\\nLoading ML model for flare separation...\")\n",
    "        model = FlareDecompositionModel(\n",
    "            sequence_length=settings.ML_PARAMS['sequence_length'],\n",
    "            n_features=settings.ML_PARAMS['n_features'],\n",
    "            max_flares=settings.ML_PARAMS['max_flares']\n",
    "        )\n",
    "        model.build_model()\n",
    "        \n",
    "        # Try to load the pre-trained model\n",
    "        model_path = os.path.join(settings.MODEL_DIR, 'flare_decomposition_model')\n",
    "        model_loaded = False\n",
    "        \n",
    "        try:\n",
    "            model.load_model(model_path)\n",
    "            print(f\"Successfully loaded model from {model_path}\")\n",
    "            model_loaded = True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            print(\"Training a simple model with synthetic data...\")\n",
    "            \n",
    "            # Generate synthetic training data\n",
    "            X_train, y_train = model.generate_synthetic_data(n_samples=500, noise_level=0.05)\n",
    "            X_val, y_val = model.generate_synthetic_data(n_samples=100, noise_level=0.05)\n",
    "            \n",
    "            # Train model\n",
    "            history = model.train(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=10,  # Using fewer epochs for demonstration\n",
    "                batch_size=settings.ML_PARAMS['batch_size'],\n",
    "                save_path=model_path\n",
    "            )\n",
    "            model_loaded = True\n",
    "        \n",
    "        if model_loaded and 'df' in locals():\n",
    "            # Process overlapping flares\n",
    "            print(\"\\nProcessing overlapping flare pairs:\")\n",
    "            \n",
    "            # Choose one overlapping pair for demonstration\n",
    "            i, j, duration = overlapping[0]\n",
    "            print(f\"Analyzing overlap between flares {i+1} and {j+1} (overlap: {duration})\")\n",
    "            \n",
    "            # Extract the time series segment\n",
    "            start_idx = min(detected_flares.iloc[i]['start_index'], detected_flares.iloc[j]['start_index'])\n",
    "            end_idx = max(detected_flares.iloc[i]['end_index'], detected_flares.iloc[j]['end_index'])\n",
    "            \n",
    "            # Ensure we have enough context around the flares\n",
    "            padding = settings.ML_PARAMS['sequence_length'] // 4\n",
    "            start_idx = max(0, start_idx - padding)\n",
    "            end_idx = min(len(df) - 1, end_idx + padding)\n",
    "            \n",
    "            # Extract the time series segment\n",
    "            segment = df.iloc[start_idx:end_idx][flux_col].values\n",
    "            \n",
    "            # Ensure the segment has the required length for the model\n",
    "            if len(segment) < settings.ML_PARAMS['sequence_length']:\n",
    "                # Pad if too short\n",
    "                segment = np.pad(segment, \n",
    "                                (0, settings.ML_PARAMS['sequence_length'] - len(segment)), \n",
    "                                'constant')\n",
    "            elif len(segment) > settings.ML_PARAMS['sequence_length']:\n",
    "                # Truncate if too long\n",
    "                segment = segment[:settings.ML_PARAMS['sequence_length']]\n",
    "            \n",
    "            # Reshape for model input\n",
    "            segment = segment.reshape(1, -1, 1)\n",
    "            \n",
    "            # Decompose the flares\n",
    "            original, individual_flares, combined = reconstruct_flares(\n",
    "                model, segment, window_size=settings.ML_PARAMS['sequence_length'], plot=True\n",
    "            )\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Calculate energy for each separated flare\n",
    "            print(\"\\nEnergy estimates for separated flares:\")\n",
    "            energies = []\n",
    "            \n",
    "            for k in range(individual_flares.shape[1]):\n",
    "                if np.max(individual_flares[:, k]) > 0.05 * np.max(original):\n",
    "                    energy = np.trapz(individual_flares[:, k])\n",
    "                    energies.append(energy)\n",
    "                    print(f\"  Flare component {k+1}: {energy:.4e}\")\n",
    "            \n",
    "            # Compare with original flares\n",
    "            print(\"\\nComparing with original flares:\")\n",
    "            for idx, flare_idx in enumerate([i, j]):\n",
    "                flare = detected_flares.iloc[flare_idx]\n",
    "                start = flare['start_time']\n",
    "                end = flare['end_time']\n",
    "                peak_flux = flare['peak_flux']\n",
    "                print(f\"  Original flare {flare_idx+1}: start={start}, end={end}, peak_flux={peak_flux:.2e}\")\n",
    "            \n",
    "            # Pseudo-validation: check if the sum of separated energies is close to the sum of original energies\n",
    "            if len(energies) > 0 and i < len(detected_flares) and j < len(detected_flares):\n",
    "                try:\n",
    "                    # Calculate energy for original flares (crude approximation)\n",
    "                    flare1 = detected_flares.iloc[i]\n",
    "                    flare2 = detected_flares.iloc[j]\n",
    "                    \n",
    "                    # Crude energy estimate based on triangle approximation\n",
    "                    orig_energy1 = 0.5 * flare1['peak_flux'] * ((flare1['end_time'] - flare1['start_time']).total_seconds() / 60)\n",
    "                    orig_energy2 = 0.5 * flare2['peak_flux'] * ((flare2['end_time'] - flare2['start_time']).total_seconds() / 60)\n",
    "                    \n",
    "                    print(f\"\\nSum of original energies (crude estimate): {orig_energy1 + orig_energy2:.4e}\")\n",
    "                    print(f\"Sum of separated energies: {sum(energies):.4e}\")\n",
    "                    \n",
    "                    # Calculate ratio\n",
    "                    ratio = sum(energies) / (orig_energy1 + orig_energy2)\n",
    "                    print(f\"Ratio (separated/original): {ratio:.2f}\")\n",
    "                    \n",
    "                    if 0.5 <= ratio <= 2.0:\n",
    "                        print(\"Energy conservation is reasonable\")\n",
    "                    else:\n",
    "                        print(\"Energy conservation may be an issue\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in energy comparison: {e}\")\n",
    "    else:\n",
    "        print(\"No overlapping flares to analyze\")\n",
    "else:\n",
    "    print(\"No detected flares available for overlapping analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc5df84",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've explored how to validate our flare detection and separation methods against known flare catalogs. We've:\n",
    "\n",
    "1. Downloaded and analyzed the NOAA SWPC flare catalog\n",
    "2. Compared our detected flares with the catalog entries\n",
    "3. Calculated detection performance metrics (precision, recall, F1-score)\n",
    "4. Analyzed detection performance by flare class\n",
    "5. Investigated false positives and false negatives\n",
    "6. Validated our ML-based flare separation approach on overlapping flares\n",
    "\n",
    "This validation process helps us understand the strengths and limitations of our methods and suggests areas for improvement. The results can be used to refine detection parameters, improve the ML model, or develop hybrid approaches that combine the best aspects of multiple methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goesflareenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
